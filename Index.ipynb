{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkm1adZgzpkh","executionInfo":{"status":"ok","timestamp":1725315791886,"user_tz":240,"elapsed":24322,"user":{"displayName":"Vinitra Muralikrishnan","userId":"14239738625620892639"}},"outputId":"24f5c134-5308-43a5-9a36-399c9bc43412"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# move into project directory\n","repo_name = \"flower-fgvc\"\n","%cd /content/drive/MyDrive/Personal-Projects/$repo_name\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ikV8idO-zuFX","executionInfo":{"status":"ok","timestamp":1725315791886,"user_tz":240,"elapsed":20,"user":{"displayName":"Vinitra Muralikrishnan","userId":"14239738625620892639"}},"outputId":"78ebabfc-ad6d-4273-c17e-202c469bcb27"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Personal-Projects/flower-fgvc\n","common\t     data\texperiments  index.py  README.md\n","config.yaml  datautils\tIndex.ipynb  models    run.yaml\n"]}]},{"cell_type":"code","source":["# set up environment\n","# comment out if not required\n","'''\n","!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","!pip install matplotlib numpy pandas pyyaml opencv-python\n","'''\n","\n","#!pip install transformers\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"8u-0KeBez7WX","executionInfo":{"status":"ok","timestamp":1725315791887,"user_tz":240,"elapsed":13,"user":{"displayName":"Vinitra Muralikrishnan","userId":"14239738625620892639"}},"outputId":"cce03af7-513b-42f8-bb36-fa43f0be98cf"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\\n!pip install matplotlib numpy pandas pyyaml opencv-python\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# this cell is for downloading data.\n","# as of yet data is not hosted and is available in the private data folder\n","\n","#!tar xf data/102flowers.tgz -C data/"],"metadata":{"id":"jDO9ey2Uz8Le","executionInfo":{"status":"ok","timestamp":1725315792115,"user_tz":240,"elapsed":238,"user":{"displayName":"Vinitra Muralikrishnan","userId":"14239738625620892639"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"VkNlM5sUzmsR","executionInfo":{"status":"ok","timestamp":1725315802483,"user_tz":240,"elapsed":10373,"user":{"displayName":"Vinitra Muralikrishnan","userId":"14239738625620892639"}}},"outputs":[],"source":["#set up some imports\n","\n","import numpy as np\n","import torch\n","import random\n","from torchvision import transforms\n","\n","# custom imports\n","\n","from common.utils import init_config, get_exp_params\n","from datautils.dataset import FlowerDataset\n","from datautils.datareader import get_file_paths"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"JuknlEGUzmsU","executionInfo":{"status":"ok","timestamp":1725315802485,"user_tz":240,"elapsed":20,"user":{"displayName":"Vinitra Muralikrishnan","userId":"14239738625620892639"}}},"outputs":[],"source":["seed = 123\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6OWsEeMBzmsU","executionInfo":{"status":"ok","timestamp":1725315802485,"user_tz":240,"elapsed":18,"user":{"displayName":"Vinitra Muralikrishnan","userId":"14239738625620892639"}},"outputId":"8f340be3-7057-4bc0-b378-4978ab202cd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["nb {'data_dir': '/content/drive/MyDrive/Personal-Projects/flower-fgvc/data', 'device': 'cpu', 'output_dir': '/content/drive/MyDrive/Personal-Projects/flower-fgvc/output', 'root_dir': '/content/drive/MyDrive/Personal-Projects/flower-fgvc', 'use_gpu': False}\n"]}],"source":["config_params = init_config()\n","print('nb', config_params)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fzHibw0hzmsW","executionInfo":{"status":"ok","timestamp":1725315802675,"user_tz":240,"elapsed":203,"user":{"displayName":"Vinitra Muralikrishnan","userId":"14239738625620892639"}},"outputId":"a120401c-4bc4-46cf-fd7e-160634a35048"},"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment parameters\n","\n","{'transform': {'resize_dim': 256, 'crop_dim': 224}, 'train': {'batch_size': 32, 'loss': 'cross-entropy', 'epoch_interval': 1, 'num_epochs': 1}, 'model': {'name': 'alexnet', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 1e-07, 'amsgrad': True, 'momentum': 0.8, 'build_on_pretrained': False, 'pretrained_filename': '/models/checkpoints/last_model.pt'}, 'dataset': {'size': 'subset'}}\n"]}],"source":["# read experiment params\n","\n","exp_params = get_exp_params()\n","print('Experiment parameters\\n')\n","print(exp_params)"]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","import os\n","from scipy.io import loadmat\n","import torch\n","from torchvision.io import read_image\n","\n","\n","class FlowerDataset(Dataset):\n","\n","    def __init__(self, data_dir, data_filepaths, transforms):\n","        img_dir = os.path.join(data_dir, 'jpg')\n","        #seg_mask_dir = os.path.join(data_dir, 'segmim')\n","        labels_path = os.path.join(data_dir, 'imagelabels.mat')\n","        labels_mat = loadmat(labels_path)\n","        #id_path = os.path.join(data_dir, 'setid.mat')\n","        #ids = loadmat(id_path)\n","        #print(ids)\n","        self.data_filepaths = data_filepaths\n","        self.img_dir = img_dir\n","        self.labels_tensor = torch.from_numpy(labels_mat['labels'][0]).int() - 1\n","        self.num_classes = len(self.labels_tensor.unique())\n","        self.data_transform = transforms\n","        #print('unique ids', ids['tstid'].min(), ids['tstid'].max())\n","\n","    def __len__(self):\n","        return len(self.data_filepaths)\n","\n","    def __getitem__(self, idx):\n","        fn = self.data_filepaths[idx]\n","        si = fn.find(\"_\")\n","        img_idx = int(fn[si+1:si+6])\n","        img_tensor = read_image(os.path.join(self.img_dir, fn)).float()\n","        img_tensor = self.data_transform(img_tensor)\n","        label = self.labels_tensor[img_idx]\n","        onehot_label = torch.zeros(self.num_classes, dtype=torch.float)\n","        onehot_label[label] = 1\n","        soft_label = torch.zeros(self.num_classes, dtype=torch.float)\n","\n","        return {\n","            'img': img_tensor,\n","            'label': label,\n","            'olabel': onehot_label,\n","            'slabel': soft_label\n","        }\n","\n"],"metadata":{"id":"4qLNMQTwHSMn","executionInfo":{"status":"ok","timestamp":1725315802675,"user_tz":240,"elapsed":8,"user":{"displayName":"Vinitra Muralikrishnan","userId":"14239738625620892639"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tDJp0hsrzmsW","executionInfo":{"status":"ok","timestamp":1725315803589,"user_tz":240,"elapsed":921,"user":{"displayName":"Vinitra Muralikrishnan","userId":"14239738625620892639"}},"outputId":"489448eb-b9d6-41e4-fd65-a92637b2b0df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Full train dataset length 1020\n","Subset train dataset length 102\n","\n","Full validation dataset length 1020\n","Subset validation dataset length 102\n","\n","Full test dataset length 6149\n","Subset test dataset length 61\n"]}],"source":["composed_transforms =  transforms.Compose([\n","            #transforms.ToTensor(),\n","            #transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                        #std=[0.229, 0.224, 0.225]),\n","            transforms.Resize(exp_params['transform']['resize_dim']),\n","            transforms.CenterCrop(exp_params['transform']['crop_dim'])\n","        ])\n","\n","train_fns, val_fns, test_fns, _ = get_file_paths(config_params['data_dir'])\n","ftr_dataset = FlowerDataset(config_params['data_dir'], train_fns, composed_transforms)\n","val_dataset = FlowerDataset(config_params['data_dir'], val_fns, composed_transforms)\n","test_dataset = FlowerDataset(config_params['data_dir'], test_fns, composed_transforms)\n","sm_trlen = int(0.1 * len(ftr_dataset))\n","sm_telen = int(0.01 * len(test_dataset))\n","sm_vlen = int(0.1 * len(val_dataset))\n","\n","sm_ftr_dataset = torch.utils.data.Subset(ftr_dataset, list(range(sm_trlen)))\n","sm_val_dataset = torch.utils.data.Subset(val_dataset, list(range(sm_vlen)))\n","sm_test_dataset = torch.utils.data.Subset(test_dataset, list(range(sm_telen)))\n","\n","print('Full train dataset length', len(ftr_dataset))\n","print('Subset train dataset length', sm_trlen)\n","print('\\nFull validation dataset length', len(val_dataset))\n","print('Subset validation dataset length', sm_vlen)\n","print('\\nFull test dataset length', len(test_dataset))\n","print('Subset test dataset length', sm_telen)"]},{"cell_type":"code","source":["'''\n","from torch.utils.data import DataLoader\n","import requests\n","from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline\n","import matplotlib.pyplot as plt\n","\n","processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","'''\n","\n","'''\n","generator = pipeline(\"text-generation\", model=\"gpt2\")\n","prompt = f\"Describe the characteristics of the flower species named sunflower\"\n","description = generator(prompt, max_length = 100, num_return_sequences=1, truncation = True)\n","print(description[0]['generated_text'])\n","print('\\n\\n')\n","'''\n","\n","'''\n","train_loader = DataLoader(sm_ftr_dataset, batch_size = 1, shuffle = False)\n","\n","# conditional image captioning\n","text = \"\"\n","plt.axis(\"off\")\n","for bi, batch in enumerate(train_loader):\n","    img = batch['img'].float().to(config_params['device']) / 255.0\n","    print(batch['label'])\n","    prompt = f\"Photograph of the flower pink primrose\"\n","    np_img = img.transpose(1, 3).transpose(1, 2).numpy()\n","    inputs = processor(np_img[0], return_tensors=\"pt\")\n","    plt.imshow(np_img[0])\n","    plt.show()\n","    out = model.generate(**inputs)\n","    print(processor.decode(out[0], skip_special_tokens = True))\n","    if bi == 0:\n","        break\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"tioAtg6tCgiF","executionInfo":{"status":"ok","timestamp":1725315803591,"user_tz":240,"elapsed":12,"user":{"displayName":"Vinitra Muralikrishnan","userId":"14239738625620892639"}},"outputId":"bca4fbeb-4d8c-488f-a411-d307d186a14d"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ntrain_loader = DataLoader(sm_ftr_dataset, batch_size = 1, shuffle = False)\\n\\n# conditional image captioning\\ntext = \"\"\\nplt.axis(\"off\")\\nfor bi, batch in enumerate(train_loader):\\n    img = batch[\\'img\\'].float().to(config_params[\\'device\\']) / 255.0\\n    print(batch[\\'label\\'])\\n    prompt = f\"Photograph of the flower pink primrose\"\\n    np_img = img.transpose(1, 3).transpose(1, 2).numpy()\\n    inputs = processor(np_img[0], return_tensors=\"pt\")\\n    plt.imshow(np_img[0])\\n    plt.show()\\n    out = model.generate(**inputs)\\n    print(processor.decode(out[0], skip_special_tokens = True))\\n    if bi == 0:\\n        break\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["\n","from models.custom_models import get_model\n","import torch\n","from common.utils import get_exp_params, get_config, save_experiment_output, save_model_helpers, save_model_chkpt\n","from torch.utils.data import DataLoader, Subset\n","from tqdm import tqdm\n","import os\n","from common.loss_utils import ECCLoss\n","\n","class Classification:\n","\n","    def __init__(self, train_dataset, val_dataset, test_dataset):\n","        cfg = get_config()\n","        self.train_dataset = train_dataset\n","        self.val_dataset = val_dataset\n","        self.test_dataset = test_dataset\n","        self.exp_params = get_exp_params()\n","        self.model_params = self.exp_params['model']\n","        self.device = cfg['device']\n","        self.num_classes = 102\n","        #self.data_transform = transforms\n","\n","    def __loss_fn(self, loss_name = 'cross-entropy'):\n","        if loss_name == 'cross-entropy':\n","            return torch.nn.CrossEntropyLoss()\n","        elif loss_name == 'mse':\n","            return torch.nn.MSELoss()\n","        elif loss_name == 'l1':\n","            return torch.nn.L1Loss()\n","        elif loss_name == 'ecc':\n","            loss_fn = ECCLoss(self.num_classes, self.dim)\n","            return loss_fn\n","        else:\n","            raise SystemExit(\"Error: no valid loss function name passed! Check run.yaml\")\n","\n","    def __save_model_checkpoint(self, model_state, optimizer_state, chkpt_info):\n","        save_experiment_output(model_state, chkpt_info, False)\n","        save_model_helpers(optimizer_state, True)\n","        #os.remove(os.path.join(self.root_dir, \"models/checkpoints/current_model.pt\"))\n","\n","    def __conduct_training(self, model, optimizer, train_loader, val_loader, tr_len, val_len):\n","        num_epochs = self.exp_params['train']['num_epochs']\n","        epoch_interval = self.exp_params['train']['epoch_interval']\n","        loss_fn = self.__loss_fn()\n","        trlosshistory, vallosshistory, valacchistory = [], [], []\n","\n","        for epoch in range(num_epochs):\n","\n","            model.train()\n","            tr_loss, val_loss, val_acc = 0.0, 0.0, 0.0\n","\n","            for _, batch in enumerate(tqdm(train_loader, desc = '\\t\\tRunning through training set', position = 0, leave = True, disable = True)):\n","                optimizer.zero_grad()\n","                imgs = batch['img'].float().to(self.device)\n","                olabels = batch['olabel']\n","                op,feats = model(imgs)\n","                print('op sz', op.size(), olabels.size(), feats.size())\n","                loss = loss_fn(op, olabels)\n","                loss.backward()\n","                optimizer.step()\n","                tr_loss += (loss.item() * imgs.size(0))\n","\n","\n","            tr_loss /= tr_len\n","            trlosshistory.append(tr_loss)\n","\n","            model.eval()\n","\n","            for _, batch in enumerate(tqdm(val_loader, desc = '\\t\\tRunning through validation set', position = 0, leave = True, disable = True)):\n","                imgs = batch['img'].float().to(self.device)\n","                olabels = batch['olabel']\n","                op = model(imgs)\n","                loss = loss_fn(op, olabels)\n","                val_loss += (loss.item() * imgs.size(0))\n","                correct_label = batch['label']\n","                pred_label = torch.argmax(op, 1)\n","                #print('label size', correct_label.size(), pred_label.size())\n","                val_acc += (correct_label == pred_label).sum()\n","\n","            val_loss /= val_len\n","            val_acc /=  val_len\n","            vallosshistory.append(val_loss)\n","            valacchistory.append(val_acc.item())\n","\n","            if epoch % epoch_interval == 0:\n","                print(f'\\tEpoch {epoch+1} Training Loss: {tr_loss}')\n","                print(f\"\\tEpoch {epoch+1} Validation Loss: {val_loss}\\n\")\n","\n","        model_info = {\n","            'trlosshistory': trlosshistory,\n","            'vallosshistory': vallosshistory,\n","            'valacchistory': valacchistory,\n","            'last_epoch': -1\n","        }\n","        self.__save_model_checkpoint(model, optimizer, model_info)\n","\n","\n","    def run_fgvc_pipeline(self):\n","        print('entered pipeline')\n","        model_name = self.model_params['name']\n","        model = get_model(102, model_name)\n","        print('got model')\n","        self.dim = model.dim\n","        optimizer = torch.optim.Adam(model.parameters(),\n","            lr = self.model_params['lr'],\n","            weight_decay = self.model_params['weight_decay'],\n","            amsgrad = self.model_params['amsgrad'])\n","        print('got optimizer')\n","        batch_size = self.exp_params['train']['batch_size']\n","\n","        train_loader = DataLoader(self.train_dataset, batch_size = batch_size, shuffle = False)\n","        val_loader = DataLoader(self.val_dataset, batch_size = batch_size, shuffle = False)\n","        tr_len = len(self.train_dataset)\n","        val_len = len(self.val_dataset)\n","\n","        print('Training of classifier...\\n')\n","\n","        self.__conduct_training(model, optimizer, train_loader, val_loader, tr_len, val_len)\n","\n","        torch.cuda.empty_cache()\n"],"metadata":{"id":"usHaGsLI90LQ","executionInfo":{"status":"ok","timestamp":1725315804785,"user_tz":240,"elapsed":1204,"user":{"displayName":"Vinitra Muralikrishnan","userId":"14239738625620892639"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["composed_transforms =  transforms.Compose([\n","            #transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                        std=[0.229, 0.224, 0.225]),\n","            transforms.Resize(exp_params['transform']['resize_dim']),\n","            transforms.CenterCrop(exp_params['transform']['crop_dim'])\n","        ])\n","classification = Classification(sm_ftr_dataset, sm_val_dataset, sm_test_dataset)\n","classification.run_fgvc_pipeline()"],"metadata":{"id":"X6RpmEbz94kl"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}